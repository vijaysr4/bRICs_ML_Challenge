{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iMwp9wMZMBtJ",
        "l2k0q0h-MXPO",
        "Up9JvxMbRKI0",
        "ZYzeAFY4nboT"
      ],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment with cutting the dataset and using PCA"
      ],
      "metadata": {
        "id": "nuZfxue4U06D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports:"
      ],
      "metadata": {
        "id": "iMwp9wMZMBtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bisect\n",
        "import os\n",
        "import re\n",
        "import gensim.downloader as api\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PR1eAq3U0lM",
        "outputId": "40af83f3-a8fd-4110-9686-97f8d8237fc4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing:"
      ],
      "metadata": {
        "id": "l2k0q0h-MXPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"glove-twitter-200.model\"):\n",
        "    embeddings_model = api.load(\"glove-twitter-200\")\n",
        "    embeddings_model.save(\"glove-twitter-200.model\")\n",
        "else:\n",
        "    embeddings_model = KeyedVectors.load(\"glove-twitter-200.model\")\n",
        "\n",
        "vector_size = 200\n",
        "\n",
        "stopwords_list = []\n",
        "with open(\"stop_words.txt\", 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        w = line.strip()\n",
        "        if w:\n",
        "            stopwords_list.append(w)\n",
        "stopwords_list = sorted(stopwords_list)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def contains_url(tweet):\n",
        "    return len(re.findall(r\"http[s]?\\S+\", tweet)) != 0\n",
        "\n",
        "def is_retweet(tweet):\n",
        "    return len(re.findall(r\"rt @?[a-zA-Z0-9_]+:? .*\", tweet)) != 0\n",
        "\n",
        "def contains_username(tweet):\n",
        "    return '@' in tweet\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    tweet = tweet.lower().strip()\n",
        "\n",
        "    if contains_url(tweet):\n",
        "        return None\n",
        "\n",
        "    if is_retweet(tweet):\n",
        "        return None\n",
        "\n",
        "    if contains_username(tweet):\n",
        "        return None\n",
        "\n",
        "    tweet = re.sub(r'\\W+', ' ', tweet)\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
        "\n",
        "    if not tweet:\n",
        "        return None\n",
        "\n",
        "    words = []\n",
        "    for word in tweet.split(\" \"):\n",
        "        pos = bisect.bisect(stopwords_list, word)\n",
        "        if pos > 0 and stopwords_list[pos - 1] == word:\n",
        "            continue\n",
        "        words.append(word)\n",
        "\n",
        "    if not words:\n",
        "        return None\n",
        "\n",
        "    words = [stemmer.stem(w) for w in words]\n",
        "\n",
        "    if not words:\n",
        "        return None\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def get_avg_embedding(tweet, model, vector_size=200):\n",
        "    words = tweet.split()\n",
        "    word_vectors = [model[word] for word in words if word in model]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean(word_vectors, axis=0)"
      ],
      "metadata": {
        "id": "YTFa2-9hWZDv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cut Dataset Experiment"
      ],
      "metadata": {
        "id": "Up9JvxMbRKI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "li = []\n",
        "for filename in os.listdir(\"/content/train_tweets\"):\n",
        "    train_df = pd.read_csv(os.path.join(\"/content/train_tweets\", filename))\n",
        "    li.append(train_df)\n",
        "train_df = pd.concat(li, ignore_index=True)\n",
        "\n",
        "train_df['Tweet'] = train_df['Tweet'].apply(preprocess_tweet)\n",
        "train_df = train_df[train_df['Tweet'].notna()].copy()\n",
        "train_df = train_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "9IBZ_-AaHJj5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Dataset with first 129 periods only:"
      ],
      "metadata": {
        "id": "YPTiwXIDnNta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows with PeriodID > 127\n",
        "train_df = train_df[train_df['PeriodID'] <= 127].copy()\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "vector_size = 200\n",
        "tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in train_df['Tweet']])\n",
        "tweet_df = pd.DataFrame(tweet_vectors)\n",
        "\n",
        "period_features = pd.concat([train_df, tweet_df], axis=1)\n",
        "period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
        "period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).median().reset_index()\n",
        "\n",
        "X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
        "y = period_features['EventType'].values.astype(int)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=24, stratify=y)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GOsm0wnhWh0x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_clf = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,\n",
        "    solver='lbfgs',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_forest_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n",
        "ensemble_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('logistic', logistic_clf),\n",
        "        ('random_forest', random_forest_clf)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred = ensemble_clf.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation Accuracy: {val_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK3JBPGpH-jD",
        "outputId": "283c035a-0449-4d12-8950-2c5119579338"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PCA"
      ],
      "metadata": {
        "id": "ZYzeAFY4nboT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "5IhEfeXOM1c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "li = []\n",
        "for filename in os.listdir(\"/content/train_tweets\"):\n",
        "    train_df = pd.read_csv(os.path.join(\"/content/train_tweets\", filename))\n",
        "    li.append(train_df)\n",
        "train_df = pd.concat(li, ignore_index=True)\n",
        "\n",
        "train_df['Tweet'] = train_df['Tweet'].apply(preprocess_tweet)\n",
        "train_df = train_df[train_df['Tweet'].notna()].copy()\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "vector_size = 200\n",
        "tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in train_df['Tweet']])\n",
        "tweet_df = pd.DataFrame(tweet_vectors)\n",
        "period_features = pd.concat([train_df, tweet_df], axis=1)\n",
        "\n",
        "period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
        "\n",
        "period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).median().reset_index()"
      ],
      "metadata": {
        "id": "pjlj5NusLKad"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "q0mOvr6IQpCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
        "y = period_features['EventType'].values.astype(int)\n",
        "\n",
        "# Standardize features before applying PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Original number of features: {X.shape[1]}\")\n",
        "print(f\"Reduced number of features after PCA: {X_pca.shape[1]}\")\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_pca, y, test_size=0.3, random_state=24, stratify=y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsZmQ9M7na9f",
        "outputId": "14e2f75b-3f18-4e53-8c5b-3a4399284c2f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 200\n",
            "Reduced number of features after PCA: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We can see that 29 features are enough to capture 95% of the variance."
      ],
      "metadata": {
        "id": "qDiNIMJ1NBwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_clf = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,\n",
        "    solver='lbfgs',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_forest_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n",
        "ensemble_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('logistic', logistic_clf),\n",
        "        ('random_forest', random_forest_clf)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred = ensemble_clf.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation Accuracy: {val_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2upDH6h3YS-8",
        "outputId": "8307d9c1-4671-46a8-eb2f-4c4016f72b56"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No PCA"
      ],
      "metadata": {
        "id": "vyZB8-9KPcm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
        "y = period_features['EventType'].values.astype(int)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=24, stratify=y)\n",
        "\n",
        "logistic_clf = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,\n",
        "    solver='lbfgs',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_forest_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "ensemble_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('logistic', logistic_clf),\n",
        "        ('random_forest', random_forest_clf)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred = ensemble_clf.predict(X_val)\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation Accuracy: {val_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc4kozSUOLEI",
        "outputId": "cebe4dc9-4ad4-443b-fe60-24019b94ba53"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we compare the test results with and without PCA, we observe a slight drop in accuracy when using PCA. However, PCA significantly reduces the number of features, leading to more efficient computations."
      ],
      "metadata": {
        "id": "xbNhkYaDQxUD"
      }
    }
  ]
}